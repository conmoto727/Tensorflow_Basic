{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imresize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "cwd = os.getcwd()\n",
    "# Folder Locations\n",
    "train_paths = [\"/DataSet/Train/Cat\"\n",
    "               ,\"/DataSet/Train/Dog\"]\n",
    "test_paths = [\"/DataSet/Test/Cat\"\n",
    "              ,\"/DataSet/Test/Dog\"]\n",
    "categories = ['Cat', 'Dog']\n",
    "# Config\n",
    "imgsize = [128,128]\n",
    "data_name = \"custom_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n",
      "(50,)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "#Path Array\n",
    "nclass = 2\n",
    "\n",
    "for i in range(nclass):\n",
    "    path = cwd + \"/\" + train_paths[i]\n",
    "    flist = os.listdir(path)\n",
    "    print(np.array(flist).shape)\n",
    "    \n",
    "for i in range(nclass):\n",
    "    path = cwd + \"/\" + test_paths[i]\n",
    "    flist = os.listdir(path)\n",
    "    print(np.array(flist).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1000 Train images loaded.\n"
     ]
    }
   ],
   "source": [
    "#Create Train Image Array\n",
    "train_cnt = 0\n",
    "\n",
    "for i in range(nclass):\n",
    "    path = cwd + \"/\" + train_paths[i]\n",
    "    flist = os.listdir(path)\n",
    "    for f in flist:\n",
    "        fullpath = os.path.join(path, f)\n",
    "        #Load Image to gray scale\n",
    "        currimg = imread(fullpath, flatten=\"gray\")\n",
    "        #reshape and rgb float\n",
    "        graysmall = imresize(currimg, [imgsize[0], imgsize[1]])/255. \n",
    "        grayvec = np.reshape(graysmall, (1, -1))\n",
    "        #Train Dataset\n",
    "        if train_cnt is 0:\n",
    "            trainimg = grayvec\n",
    "        else:\n",
    "            trainimg = np.concatenate((trainimg, grayvec), axis=0)\n",
    "        train_cnt = train_cnt + 1\n",
    "\n",
    "print (\"Total %d Train images loaded.\" % (train_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " type of trainimg <class 'numpy.ndarray'>\n",
      " type of trainlabel <class 'numpy.ndarray'>\n",
      "(1000, 16384)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Create Train Label\n",
    "trainlabel = np.array([1, 0]).reshape((1,2))\n",
    "\n",
    "#Cat Label is [1 0]\n",
    "for i in range(499):\n",
    "    label = np.array([1, 0]).reshape((1,2))\n",
    "    trainlabel = np.concatenate((trainlabel, label), axis=0)\n",
    "    \n",
    "#Dog Label is [0 1]\n",
    "for i in range(500):\n",
    "    label = np.array([0, 1]).reshape((1,2))\n",
    "    trainlabel = np.concatenate((trainlabel, label), axis=0)\n",
    "\n",
    "print(\" type of trainimg %s\"%(type(trainimg)))\n",
    "print(\" type of trainlabel %s\" %(type(trainlabel)))\n",
    "print(trainimg.shape)\n",
    "print(trainlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 100 Test images loaded.\n"
     ]
    }
   ],
   "source": [
    "#Create Test Image Array\n",
    "test_cnt = 0\n",
    "\n",
    "for i in range(nclass):\n",
    "    path = cwd + test_paths[i]\n",
    "    flist = os.listdir(path)\n",
    "    for f in flist:\n",
    "        fullpath = os.path.join(path, f)\n",
    "        currimg = imread(fullpath, flatten=\"gray\")\n",
    "        \n",
    "        #reshape and rgb float\n",
    "        graysmall = imresize(currimg, [imgsize[0], imgsize[1]])/255. \n",
    "        grayvec = np.reshape(graysmall, (1, -1))\n",
    "        #Test Dataset\n",
    "        if test_cnt is 0:\n",
    "            testimg = grayvec\n",
    "        else:\n",
    "            testimg = np.concatenate((testimg, grayvec), axis = 0)\n",
    "        test_cnt = test_cnt + 1\n",
    "\n",
    "print(\"Total %d Test images loaded.\"%(test_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16384)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "#Create Test Label\n",
    "testlabel = np.array([1, 0]).reshape((1,2))\n",
    "# Cat\n",
    "for i in range(49):\n",
    "    label = np.array([1, 0]).reshape((1,2))\n",
    "    testlabel = np.concatenate((testlabel, label), axis=0)\n",
    "# Dog\n",
    "for i in range(50):\n",
    "    label = np.array([0, 1]).reshape((1,2))\n",
    "    testlabel = np.concatenate((testlabel, label), axis=0)\n",
    "    \n",
    "print(testimg.shape)\n",
    "print(testlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "# NETWORK TOPOLOGIES\n",
    "n_input    = imgsize[0]*imgsize[1]\n",
    "n_channel  = 64\n",
    "n_output   = 2\n",
    "\n",
    "# INPUTS AND OUTPUTS\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_output])\n",
    "    \n",
    "# Weights (width, height, channel, number)\n",
    "weights = {\n",
    "    'conv_w': tf.Variable(tf.random_normal([3,3,1,n_channel], stddev=0.1)),\n",
    "    'dense_w': tf.Variable(tf.random_normal([64*64*n_channel, n_output], stddev=0.1))\n",
    "}\n",
    "\n",
    "# Biases\n",
    "biases = {\n",
    "    'conv_b': tf.Variable(tf.random_normal([n_channel], stddev=0.1)),\n",
    "    'dense_b': tf.Variable(tf.random_normal([n_output], stddev=0.1))\n",
    "}\n",
    "\n",
    "print (\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONS READY\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "def CNN(_input, _w, _b):\n",
    "    # RESHAPE\n",
    "    _input_r = tf.reshape(_input, shape=[-1, imgsize[0], imgsize[1], 1])\n",
    "    # CONVOLUTION\n",
    "    _conv = tf.nn.conv2d(_input_r, _w['conv_w'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # ADD BIAS\n",
    "    _bias = tf.nn.bias_add(_conv, _b['conv_b'])\n",
    "    # RELU\n",
    "    _relu = tf.nn.relu(_bias)\n",
    "    # MAX-POOL\n",
    "    _pool  = tf.nn.max_pool(_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # VECTORIZE\n",
    "    _dense = tf.reshape(_pool, [-1, _w['dense_w'].get_shape().as_list()[0]])\n",
    "    # DENSE\n",
    "    _logit = tf.add(tf.matmul(_dense, _w['dense_w']), _b['dense_b'])\n",
    "    _out = {\n",
    "        'input_r': _input_r, 'conv': _conv, 'biase': _bias, 'relu': _relu\n",
    "        , 'pool': _pool, 'dense': _dense, 'logit': _logit\n",
    "    }\n",
    "    return _out\n",
    "\n",
    "# PREDICTION\n",
    "cnn_out = CNN(x, weights, biases)\n",
    "\n",
    "# LOSS AND OPTIMIZER\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=cnn_out['logit']))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "corr = tf.equal(tf.argmax(cnn_out['logit'], 1), tf.argmax(y, 1))    \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# INITIALIZER\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_weigt\n",
      "(3, 3, 1, 64)\n",
      "conv_bias\n",
      "(64,)\n",
      "dense_weight\n",
      "(262144, 2)\n",
      "dense_bias\n",
      "(2,)\n",
      "Save Ready.\n"
     ]
    }
   ],
   "source": [
    "saver_cw = tf.train.Saver([weights['conv_w']])\n",
    "print(\"conv_weigt\")\n",
    "print(weights['conv_w'].shape)\n",
    "\n",
    "saver_cb = tf.train.Saver([biases['conv_b']])\n",
    "print(\"conv_bias\")\n",
    "print(biases['conv_b'].shape)\n",
    "\n",
    "saver_dw = tf.train.Saver([weights['dense_w']])\n",
    "print(\"dense_weight\")\n",
    "print(weights['dense_w'].shape)\n",
    "\n",
    "saver_db = tf.train.Saver([biases['dense_b']])\n",
    "print(\"dense_bias\")\n",
    "print(biases['dense_b'].shape)\n",
    "\n",
    "print(\"Save Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005/025 cost: 0.873946196\n",
      "TRAIN ACCURACY: 0.636\n",
      "TEST ACCURACY: 0.640\n",
      "Epoch: 010/025 cost: 0.263016914\n",
      "TRAIN ACCURACY: 0.913\n",
      "TEST ACCURACY: 0.750\n",
      "Epoch: 015/025 cost: 0.167516857\n",
      "TRAIN ACCURACY: 0.983\n",
      "TEST ACCURACY: 0.740\n",
      "Epoch: 020/025 cost: 0.071072880\n",
      "TRAIN ACCURACY: 0.995\n",
      "TEST ACCURACY: 0.770\n",
      "Epoch: 025/025 cost: 0.043495178\n",
      "TRAIN ACCURACY: 0.996\n",
      "TEST ACCURACY: 0.790\n",
      "OPTIMIZATION FINISHED\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "training_epochs = 25\n",
    "batch_size      = 50\n",
    "display_step    = 5\n",
    "# LAUNCH THE GRAPH\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# OPTIMIZE\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_cnt/batch_size)\n",
    "    # ITERATION\n",
    "    for i in range(total_batch):\n",
    "        randidx = np.random.choice(trainimg.shape[0], size=batch_size)\n",
    "        batch_xs = trainimg[randidx,:]\n",
    "        batch_ys = trainlabel[randidx,:]\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # DISPLAY\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "        feeds = {x: trainimg, y: trainlabel}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"TRAIN ACCURACY: %.3f\" % (train_acc))\n",
    "        feeds = {x: testimg, y: testlabel}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"TEST ACCURACY: %.3f\" % (test_acc))\n",
    "    \n",
    "print (\"OPTIMIZATION FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 64)\n",
      "(64,)\n",
      "(262144, 2)\n",
      "(2,)\n",
      "Model saved in file: CNN_Save_128/dense_b.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(weights['conv_w'].shape)\n",
    "save_path = saver_cw.save(sess, \"CNN_Save_128/conv_w.ckpt\")\n",
    "\n",
    "print(biases['conv_b'].shape)\n",
    "save_path = saver_cb.save(sess, \"CNN_Save_128/conv_b.ckpt\")\n",
    "\n",
    "print(weights['dense_w'].shape)\n",
    "save_path = saver_dw.save(sess, \"CNN_Save_128/dense_w.ckpt\")\n",
    "\n",
    "print(biases['dense_b'].shape)\n",
    "save_path = saver_db.save(sess, \"CNN_Save_128/dense_b.ckpt\")\n",
    "\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
